# Notes

## 22-01-2026

- I started working on my thesis today.
- Set up the repository for the application & report.
- Decided I will use Bluetext & Polytope (software infrastructure related to my internship) to build the backbone of the experiments & analysis.
- Decided together with my internship responsible contact that I will start on a day every week of internship work. This will linearly increase to full-time at the end of the semester (July 15th).
- Did literature research. Main takeaways so far is that there have been some attempts to tackle the issues at hand using LLMs, albeit most approaches use dated language models (<GPT 3.5), or very simple zero-shot methods.

## 29-01-2026

- I want to start outlining the sections of the thesis report, and just iterate over them whenever I have new learnings. 
- So far, I've done literature research into relevant papers and datasets. The results are visible in [Thesis](https://www.notion.so/jellevlieshout/Thesis-27130ab2581c80699e5ceed6a0ddcc5a?source=copy_link). 
- I think the most suitable candidate for Idioms is the SemEval 2022 Task 2 dataset, and the VU Amsterdam Metaphor corpus for metaphors. I haven't yet thought of evaluation methods much.
- I've also started setting up the environment for the project. This includes a client-facing front-end, API, and langgraph workflows + langsmith observability. In its current state, a piece of text can be entered, and is fed through a placeholder agent + prompt using openrouter. The results are then displayed on the front-end.
- I need to start thinking about research questions. So far, I'm thinking somewhere along the lines of:
    - Can an agentic application be built that can outperform state-of-the-art methods of recognising figurative expressions?
    - Can an agentic application be built that can outperform state-of-the-art methods of classifying figurative expressions?
    - Can an agentic application be built that can outperform state-of-the-art methods of replacing figurative expressions with easy-to-read text?
- See below an AI refined version of these RQ's:
    - RQ1: To what extent can a zero-/one-shot agentic LLM pipeline accurately detect and interpret idiomatic and metaphorical expressions in context, compared to established supervised benchmarks?
    - RQ2: Does an explicit agentic decomposition (detection → explanation → transformation) improve figurative language handling compared to monolithic prompt-based LLM approaches?
    - RQ3: How effectively can an agentic LLM system replace idiomatic and metaphorical expressions with literal, meaning-preserving alternatives, and what trade-offs arise between semantic fidelity and readability?
- Hypotheses:
    - H1: A zero-/one-shot agentic LLM pipeline can achieve performance comparable to supervised systems on idiom detection and metaphor identification benchmarks, despite the absence of task-specific training.
    - H2: Agentic decomposition significantly improves figurative language replacement quality compared to single-prompt LLM baselines, particularly for metaphors.
    - H3: Idiomatic expressions are more reliably replaced with meaning-preserving literal alternatives than conceptual metaphors, due to the latter’s reliance on abstract source–target mappings.
    - H4: Explicit intermediate explanations generated by the agent correlate positively with the semantic correctness of the final literal replacement.
- Suggested thesis outline:
    - Chapter 1 — Introduction
        - Motivation: figurative language as a barrier to accessibility, simplification, and downstream NLP
        - Limitations of supervised, task-specific approaches
        - Rise of LLMs and agentic reasoning
        - Contributions
            - Zero/one-shot/few-shot agentic system for figurative language handling
            - Unified treatment of idioms and metaphors (is this the case?)
            - Empirical evaluation of replacement quality
    - Chapter 2 — Background & Related Work
        - 2.1 Figurative language
            - Idioms vs metaphors (linguistic + cognitive distinction)
            - Prior work on detection, classification, and simplification (loads of papers on this)
        - 2.2 Datasets
            - SemEval-2022 Task 2 (idioms)
            - VU Amsterdam Metaphor Corpus
            - Supporting datasets (MAGPIE, TroFi, MOH)
            - Conceptual resources (MetaNet)
        - 2.3 Large Language Models & Agentic Systems
            - Zero-shot, one-shot and few-shot prompting
            - Agentic decomposition (planning, reflection, self-verification)
            - Observability (LangSmith-style traces)
    - Chapter 3 — System Design & Methodology
        - 3.1 Problem formulation
            - Input: raw text
            - Output: literalized, meaning-preserving text + intermediate explanations
        - 3.2 Agentic pipeline
            - Figurative span detection
            - Interpretation / explanation
            - Literal replacement generation
            - Self-verification & revision
            - Self-learning through addition to RAG? 
        - 3.3 Baselines
            - Monolithic single-prompt LLM
            - Detection-only prompting
            - Naïve paraphrasing
    - Chapter 4 — RQ1: Detection & Interpretation 
        - _To what extent can a zero-/one-shot agentic LLM pipeline accurately detect and interpret idiomatic and metaphorical expressions in context?_
        - Experiments
            - Idioms: SemEval-2022 (detection / idiomaticity)
            - Metaphors: VUA (token-level metaphor detection)
            - Compare:
                - Agentic pipeline
                - Single-prompt LLM
                - Reported supervised benchmarks (from literature)
            - Metrics
                - Accuracy / F1 (detection)
                - Qualitative interpretation correctness
    - Chapter 5 — RQ2: Agentic Decomposition
        - _Does agentic decomposition improve figurative language handling compared to monolithic prompting?_
        - Experiments
            - Same inputs, different architectures
            - Ablation:
                - no explanation step
                - no self-verification
                - full agentic pipeline
        - Metrics
            - Replacement quality (see Chapter 6)
            - Error types
            - Failure traceability
    - Chapter 6 — RQ3: Replacement Quality
        - _How effectively can figurative expressions be replaced with literal, meaning-preserving alternatives?_
        - Idioms
            - SemEval-2022 Subtask B–style evaluation
            - Semantic similarity to gold paraphrases
        - Metaphors
            - Custom evaluation protocol (see Section 2 below)
        - Analysis
            - Idioms vs metaphors
            - Trade-off: readability vs semantic fidelity
    - Chapter 7 — RQ4: Observability & Error Analysis
        - _How observable and debuggable are agentic systems compared to end-to-end prompting?_
        - Case studies using LangSmith traces
        - Error localization
        - Correlation between explanation quality and outcome quality
    - Chapter 8 — Discussion & Limitations
        - What agentic systems can/cannot do
        - Where metaphors fundamentally resist literalization
    - Chapter 9 — Conclusion & Future Work
        - Summary of findings
        - Implications for NLP accessibility
        - Directions: multilinguality, discourse-level metaphors, human-in-the-loop

- Random idea:
    - Build a RAG system containing previously identified figurative expressions to improve the detection of figurative language, and potentially provide the agent with more context / examples when trying to deduce meaning to replace figurative expressions.
- Known Limitations
    - Conceptual
        - Some metaphors encode meaning that cannot be fully literalized without loss
        - Replacement may oversimplify stylistic or pragmatic nuance
    - Evaluation limits
        - Human evaluation introduces subjectivity
        - No canonical gold standard for metaphor replacement
    - Model dependence
        - Results may vary across LLM providers
        - Findings concern capabilities of current LLMs, not a specific architecture
    - Scope limits
        - Sentence or MWE (multi-word-expression)-level focus
        - No discourse-wide metaphor tracking
        - Primarily English
    
| Dataset                      | Used for                        | RQ       | Role                           |
| ---------------------------- | ------------------------------- | -------- | ------------------------------ |
| SemEval-2022 Task 2          | Idiom detection & replacement   | RQ1, RQ3 | Primary quantitative benchmark |
| VU Amsterdam Metaphor Corpus | Metaphor identification         | RQ1, RQ3 | Primary metaphor dataset       |
| MAGPIE                       | Idiom interpretation robustness | RQ1      | Qualitative / stress testing   |
| TroFi                        | Verb metaphor sanity checks     | RQ3      | Focused evaluation             |
| MOH / MOH-X                  | Verb metaphor detection         | RQ1      | Secondary validation           |
| MetaNet                      | Conceptual grounding            | RQ3      | Interpretation support         |

- Overall idea:
    - Structured, observable, agentic reasoning with LLMs enables reliable figurative language interpretation and transformation without task-specific training.
- Note:
    - Option + Z = rewrap text

## 05-02-2026

- Continuation of thesis work. Objectives for today are:
   - Build a client to interface with an initial dataset -> VU Amsterdam Metaphor Corpus
   - Further research and decision on evaluation methods for the figurative language replacement task. 
- The Oxford Text Archive is down; they have a legacy website up through [here](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2541). 
- The VU Amsterdam Metaphor Corpus is available as XML, which I will add to the repository. I will create a client to easily interface with the dataset.
- Most of the metaphors look less straightforward to replace than I initially thought. There's quite a lot of nuance and understanding of the English language required to replace them with literal, meaning-preserving alternatives. 
- I've added a client to interface with the VU Amsterdam Metaphor Corpus. I'm interested to see if we can easily 'merge' datasets and if their annotation methodologies are similar. I should probably dedicate a section of the report to these differences. 
- I'm also wondering what will be a good 'format' (both in and output) to present a potential metaphor to the agent, as well as how to structure the output. I'm thinking of taking inspiration from the methodology the datasets use to structure their data.
- I've implemented an agentic flow that does detection according to the following format:
```
    Canonical Metaphor Detection I/O (v1)
    
    Input: Yesterday it was raining cats and dogs.

    Task: Detect Metaphors

    Output: 
    {
      "metaphor_spans": [
        {
          "text": "raining cats and dogs",
          "char_start": 17,
          "char_end": 38,
          "confidence": 1
        }
      ]
    }
```

- I'm focussing on metaphor detection (and not replacement) for now, to build a prototype.
- I need to start thinking on how I will evaluate the replacement quality of the agent. There aren't really any datasets available for this task, so I'll need to come up with a custom evaluation protocol. 
- I have added a client & front-end interface for the SemEval-2022 Task 2 dataset, which is a dataset for idiom detection and replacement. 

- Evaluation rundown:

#### Evaluation of Figurative Language Systems

This section focuses on evaluation methodologies for figurative language processing, with emphasis on metaphor and idiom interpretation and generation. It explains commonly proposed automatic metrics, their limitations for figurative language, and outlines alternative and complementary evaluation strategies more aligned with the nature of the task.

---

##### 1. Standard Automatic Evaluation Metrics

###### 1.1 BLEU

**What it measures**
- N-gram overlap between a system output and one or more reference texts.
- Primarily precision-oriented, with a brevity penalty for short outputs.

**How it works**
- Counts overlapping n-grams (typically 1–4 grams).
- Applies a penalty if the generated output is shorter than the reference.

**Limitations for figurative language**
- Figurative paraphrases intentionally avoid lexical overlap.
- Multiple correct interpretations may share no surface n-grams.
- Poor correlation with human judgment for metaphor explanation or reinterpretation.

**Summary**
> BLEU is largely misaligned with tasks involving semantic reinterpretation and figurative meaning.

---

###### 1.2 SARI

**What it measures**
- Quality of **Add**, **Delete**, and **Keep** operations relative to the input and references.
- Originally designed for text simplification.

**How it works**
- Rewards deletion of complex expressions.
- Rewards addition of clarifying material.
- Rewards retention of essential content.

**Relevance to figurative language**
- Figurative-to-literal transformation resembles simplification.
- Encourages meaning preservation under transformation.

**Limitations**
- Assumes availability of high-quality reference simplifications.
- Penalizes valid outputs when multiple paraphrases are possible.

**Summary**
> SARI is better aligned than BLEU but still constrained by reference dependence.

---

###### 1.3 BERTScore

**What it measures**
- Semantic similarity between generated text and reference text using contextual embeddings.

**How it works**
- Computes cosine similarity between contextualized token embeddings.
- Allows soft alignment rather than exact string matches.

**Strengths**
- Robust to paraphrasing and lexical variation.
- Captures semantic proximity more effectively than n-gram metrics.

**Limitations**
- High similarity does not guarantee correct interpretation.
- Can reward plausible but incorrect explanations.
- Does not explicitly test figurative understanding.

**Summary**
> BERTScore measures semantic closeness, not interpretive correctness.

---

##### 2. Why Figurative Language Challenges Standard Metrics

Standard automatic metrics rely on assumptions that do not hold for figurative language:

| Assumption | Why it fails |
|----------|-------------|
| Single correct output | Figurative expressions admit multiple valid interpretations |
| Lexical overlap signals correctness | Figurative paraphrases often diverge lexically |
| Surface similarity implies meaning preservation | Figurative interpretation requires inference |
| References are exhaustive | Human references rarely cover the full meaning space |

As a result, automatic metrics often correlate weakly with human judgments in metaphor and idiom tasks.

---

##### 3. More Relevant Evaluation Approaches

###### 3.1 Human-Centered Evaluation

Human judgment remains essential for evaluating figurative language understanding.

###### (a) Meaning Fidelity
> Does the output preserve the intended meaning of the figurative expression?

Typical scale (1–5):
- 1: Incorrect or misleading
- 3: Partially correct
- 5: Fully faithful interpretation

###### (b) Interpretability / Clarity
> Is the explanation understandable to a non-expert reader?

###### (c) Figurative Awareness
> Does the system recognize the expression as figurative rather than literal?

This can be evaluated as:
- Binary (figurative vs. literal)
- Graded (literal / mixed / figurative)

---

###### 3.2 Span-Level and Classification Evaluation

For systems that output spans or labels:
- Precision, Recall, and F1-score over:
  - Figurative span detection
  - Metaphor vs. idiom classification

This approach aligns well with datasets such as VU Amsterdam Metaphor Corpus, MOH-X, and SemEval idiom benchmarks.

---

###### 3.3 Faithfulness and Consistency Tests

###### (a) Perturbation-Based Evaluation
- Replace figurative expressions with literal paraphrases.
- Verify whether explanations change appropriately.

##### (b) Consistency Evaluation
- Check whether identical metaphors across different contexts yield consistent core interpretations.

These tests probe conceptual understanding rather than surface-level similarity.

---

###### 3.4 LLM-as-a-Judge Evaluation

A stronger language model can be used solely as an evaluator under strict controls.

**Evaluation criteria**
- Meaning preservation
- Figurative recognition
- Hallucination detection

**Methodological safeguards**
- Fixed evaluation rubric
- Blind assessment (no model identity)
- Measurement of agreement with human annotators

When transparently reported, this approach is increasingly accepted in NLP research.

---

###### 3.5 Concept-Level Evaluation (Conceptual Metaphors)

For datasets grounded in conceptual metaphor theory (e.g., MetaNet):

Evaluate whether the system correctly identifies:
- Source domain
- Target domain
- Source–target mapping (e.g., ANGER → HEAT)

Scoring can be:
- Exact match
- Partial match
- Incorrect

This form of evaluation directly targets conceptual understanding and is underexplored in current literature.

---

##### 4. Proposed Multi-Layer Evaluation Framework

A defensible evaluation protocol for figurative language systems can be structured as follows:

###### Layer 1 – Detection Accuracy
- Span-level Precision, Recall, and F1

###### Layer 2 – Semantic Similarity (Supporting)
- BERTScore
- SARI (for figurative-to-literal transformation)

###### Layer 3 – Interpretive Correctness (Primary)
- Human evaluation of meaning fidelity
- Assessment of figurative awareness

###### Layer 4 – Conceptual Grounding
- Correctness of conceptual metaphor mappings
- Idiom sense disambiguation accuracy

---

##### 5. Contribution to the Thesis

Most figurative language research prioritizes model performance and treats evaluation as secondary. By contrast, this thesis positions evaluation as a primary contribution by:

- Demonstrating the limitations of standard automatic metrics
- Proposing task-aligned, concept-aware evaluation methods
- Empirically analyzing the gap between automatic scores and human judgment

This approach is model-agnostic, methodologically rigorous, and well-aligned with current research needs in figurative language processing.
