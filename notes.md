# Notes

## 22-01-2026

- I started working on my thesis today.
- Set up the repository for the application & report.
- Decided I will use Bluetext & Polytope (software infrastructure related to my internship) to build the backbone of the experiments & analysis.
- Decided together with my internship responsible contact that I will start on a day every week of internship work. This will linearly increase to full-time at the end of the semester (July 15th).
- Did literature research. Main takeaways so far is that there have been some attempts to tackle the issues at hand using LLMs, albeit most approaches use dated language models (<GPT 3.5), or very simple zero-shot methods.

## 29-01-2026

- I want to start outlining the sections of the thesis report, and just iterate over them whenever I have new learnings. 
- So far, I've done literature research into relevant papers and datasets. The results are visible in [Thesis](https://www.notion.so/jellevlieshout/Thesis-27130ab2581c80699e5ceed6a0ddcc5a?source=copy_link). 
- I think the most suitable candidate for Idioms is the SemEval 2022 Task 2 dataset, and the VU Amsterdam Metaphor corpus for metaphors. I haven't yet thought of evaluation methods much.
- I've also started setting up the environment for the project. This includes a client-facing front-end, API, and langgraph workflows + langsmith observability. In its current state, a piece of text can be entered, and is fed through a placeholder agent + prompt using openrouter. The results are then displayed on the front-end.
- I need to start thinking about research questions. So far, I'm thinking somewhere along the lines of:
    - Can an agentic application be built that can outperform state-of-the-art methods of recognising figurative expressions?
    - Can an agentic application be built that can outperform state-of-the-art methods of classifying figurative expressions?
    - Can an agentic application be built that can outperform state-of-the-art methods of replacing figurative expressions with easy-to-read text?
- See below an AI refined version of these RQ's:
    - RQ1: To what extent can a zero-/one-shot agentic LLM pipeline accurately detect and interpret idiomatic and metaphorical expressions in context, compared to established supervised benchmarks?
    - RQ2: Does an explicit agentic decomposition (detection → explanation → transformation) improve figurative language handling compared to monolithic prompt-based LLM approaches?
    - RQ3: How effectively can an agentic LLM system replace idiomatic and metaphorical expressions with literal, meaning-preserving alternatives, and what trade-offs arise between semantic fidelity and readability?
- Hypotheses:
    - H1: A zero-/one-shot agentic LLM pipeline can achieve performance comparable to supervised systems on idiom detection and metaphor identification benchmarks, despite the absence of task-specific training.
    - H2: Agentic decomposition significantly improves figurative language replacement quality compared to single-prompt LLM baselines, particularly for metaphors.
    - H3: Idiomatic expressions are more reliably replaced with meaning-preserving literal alternatives than conceptual metaphors, due to the latter’s reliance on abstract source–target mappings.
    - H4: Explicit intermediate explanations generated by the agent correlate positively with the semantic correctness of the final literal replacement.
- Suggested thesis outline:
    - Chapter 1 — Introduction
        - Motivation: figurative language as a barrier to accessibility, simplification, and downstream NLP
        - Limitations of supervised, task-specific approaches
        - Rise of LLMs and agentic reasoning
        - Contributions
            - Zero/one-shot/few-shot agentic system for figurative language handling
            - Unified treatment of idioms and metaphors (is this the case?)
            - Empirical evaluation of replacement quality
    - Chapter 2 — Background & Related Work
        - 2.1 Figurative language
            - Idioms vs metaphors (linguistic + cognitive distinction)
            - Prior work on detection, classification, and simplification (loads of papers on this)
        - 2.2 Datasets
            - SemEval-2022 Task 2 (idioms)
            - VU Amsterdam Metaphor Corpus
            - Supporting datasets (MAGPIE, TroFi, MOH)
            - Conceptual resources (MetaNet)
        - 2.3 Large Language Models & Agentic Systems
            - Zero-shot, one-shot and few-shot prompting
            - Agentic decomposition (planning, reflection, self-verification)
            - Observability (LangSmith-style traces)
    - Chapter 3 — System Design & Methodology
        - 3.1 Problem formulation
            - Input: raw text
            - Output: literalized, meaning-preserving text + intermediate explanations
        - 3.2 Agentic pipeline
            - Figurative span detection
            - Interpretation / explanation
            - Literal replacement generation
            - Self-verification & revision
            - Self-learning through addition to RAG? 
        - 3.3 Baselines
            - Monolithic single-prompt LLM
            - Detection-only prompting
            - Naïve paraphrasing
    - Chapter 4 — RQ1: Detection & Interpretation 
        - _To what extent can a zero-/one-shot agentic LLM pipeline accurately detect and interpret idiomatic and metaphorical expressions in context?_
        - Experiments
            - Idioms: SemEval-2022 (detection / idiomaticity)
            - Metaphors: VUA (token-level metaphor detection)
            - Compare:
                - Agentic pipeline
                - Single-prompt LLM
                - Reported supervised benchmarks (from literature)
            - Metrics
                - Accuracy / F1 (detection)
                - Qualitative interpretation correctness
    - Chapter 5 — RQ2: Agentic Decomposition
        - _Does agentic decomposition improve figurative language handling compared to monolithic prompting?_
        - Experiments
            - Same inputs, different architectures
            - Ablation:
                - no explanation step
                - no self-verification
                - full agentic pipeline
        - Metrics
            - Replacement quality (see Chapter 6)
            - Error types
            - Failure traceability
    - Chapter 6 — RQ3: Replacement Quality
        - _How effectively can figurative expressions be replaced with literal, meaning-preserving alternatives?_
        - Idioms
            - SemEval-2022 Subtask B–style evaluation
            - Semantic similarity to gold paraphrases
        - Metaphors
            - Custom evaluation protocol (see Section 2 below)
        - Analysis
            - Idioms vs metaphors
            - Trade-off: readability vs semantic fidelity
    - Chapter 7 — RQ4: Observability & Error Analysis
        - _How observable and debuggable are agentic systems compared to end-to-end prompting?_
        - Case studies using LangSmith traces
        - Error localization
        - Correlation between explanation quality and outcome quality
    - Chapter 8 — Discussion & Limitations
        - What agentic systems can/cannot do
        - Where metaphors fundamentally resist literalization
    - Chapter 9 — Conclusion & Future Work
        - Summary of findings
        - Implications for NLP accessibility
        - Directions: multilinguality, discourse-level metaphors, human-in-the-loop

- Random idea:
    - Build a RAG system containing previously identified figurative expressions to improve the detection of figurative language, and potentially provide the agent with more context / examples when trying to deduce meaning to replace figurative expressions.
- Known Limitations
    - Conceptual
        - Some metaphors encode meaning that cannot be fully literalized without loss
        - Replacement may oversimplify stylistic or pragmatic nuance
    - Evaluation limits
        - Human evaluation introduces subjectivity
        - No canonical gold standard for metaphor replacement
    - Model dependence
        - Results may vary across LLM providers
        - Findings concern capabilities of current LLMs, not a specific architecture
    - Scope limits
        - Sentence or MWE (multi-word-expression)-level focus
        - No discourse-wide metaphor tracking
        - Primarily English
    
| Dataset                      | Used for                        | RQ       | Role                           |
| ---------------------------- | ------------------------------- | -------- | ------------------------------ |
| SemEval-2022 Task 2          | Idiom detection & replacement   | RQ1, RQ3 | Primary quantitative benchmark |
| VU Amsterdam Metaphor Corpus | Metaphor identification         | RQ1, RQ3 | Primary metaphor dataset       |
| MAGPIE                       | Idiom interpretation robustness | RQ1      | Qualitative / stress testing   |
| TroFi                        | Verb metaphor sanity checks     | RQ3      | Focused evaluation             |
| MOH / MOH-X                  | Verb metaphor detection         | RQ1      | Secondary validation           |
| MetaNet                      | Conceptual grounding            | RQ3      | Interpretation support         |

- Overall idea:
    - Structured, observable, agentic reasoning with LLMs enables reliable figurative language interpretation and transformation without task-specific training.
- Note:
    - Option + Z = rewrap text

## 05-02-2026

- Continuation of thesis work. Objectives for today are:
   - Build a client to interface with an initial dataset -> VU Amsterdam Metaphor Corpus
   - Further research and decision on evaluation methods for the figurative language replacement task. 
- The Oxford Text Archive is down; they have a legacy website up through [here](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2541). 
- The VU Amsterdam Metaphor Corpus is available as XML, which I will add to the repository. I will create a client to easily interface with the dataset.
- Most of the metaphors look less straightforward to replace than I initially thought. There's quite a lot of nuance and understanding of the English language required to replace them with literal, meaning-preserving alternatives. 
- I've added a client to interface with the VU Amsterdam Metaphor Corpus. I'm interested to see if we can easily 'merge' datasets and if their annotation methodologies are similar. I should probably dedicate a section of the report to these differences. 
- I'm also wondering what will be a good 'format' (both in and output) to present a potential metaphor to the agent, as well as how to structure the output. I'm thinking of taking inspiration from the methodology the datasets use to structure their data.
- I've implemented an agentic flow that does detection according to the following format:
```
    Canonical Metaphor Detection I/O (v1)
    
    Input: Yesterday it was raining cats and dogs.

    Task: Detect Metaphors

    Output: 
    {
      "metaphor_spans": [
        {
          "text": "raining cats and dogs",
          "char_start": 17,
          "char_end": 38,
          "confidence": 1
        }
      ]
    }
```

- I'm focussing on metaphor detection (and not replacement) for now, to build a prototype.